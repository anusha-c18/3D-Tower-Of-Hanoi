{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VI72.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anusha-c18/3D-Tower-Of-Hanoi/blob/main/VI72.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpiv-5zLoG38",
        "outputId": "b02655fb-71d8-4298-aa78-5ce19487d3a4"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BXzoQQ0ZHTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37be0d42-d110-439e-a3d0-b8b061e3259c"
      },
      "source": [
        "pip install lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9lwnVM2aJ9W"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmAJPsk3arvW"
      },
      "source": [
        "scrapped_data=urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
        "article=scrapped_data.read()\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "paragraphs=parsed_article.find_all('p')\n",
        "article_text=\"\"\n",
        "for p in paragraphs:\n",
        "  article_text+=p.text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlyvpDjHcDBJ"
      },
      "source": [
        "processed_article = article_text.lower()\n",
        "processed_article=re.sub('[^a-zA-Z]',' ',processed_article)\n",
        "processed_article=re.sub(r'\\s+',' ',processed_article)\n",
        "all_sentences=nltk.sent_tokenize(processed_article)\n",
        "all_words=[nltk.word_tokenize(sent) for sent in all_sentences]\n",
        "from nltk.corpus import stopwords\n",
        "for i in range(len(all_words)):\n",
        "  all_words[i]=[w for w in all_words[i] if w not in stopwords.words('english')]\n",
        "print(all_words)  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J1Fr9f-i1zb"
      },
      "source": [
        "word embedding done below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRTrD90OjlkG"
      },
      "source": [
        "word dataset: all_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ4wHwiLjyA0"
      },
      "source": [
        "word vectors: word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsjyzfmnfaUt"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "word2vec = Word2Vec(all_words,min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOHq3yP5ioM-"
      },
      "source": [
        "retrieving the vector for a specific word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0w1bQcgH88"
      },
      "source": [
        "v1=word2vec.wv['learning']\n",
        "print(v1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhJGFe4g7hx"
      },
      "source": [
        "sim_words=word2vec.wv.most_similar('learning')\n",
        "for x in sim_words:\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}